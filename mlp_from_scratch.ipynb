{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Multi-Layer Perceptron from Scratch in NumPy\n",
    "\n",
    "**Objective:** This notebook implements a Multi-Layer Perceptron (MLP) from scratch using only NumPy. The goal is to demonstrate the mechanics of forward and backward propagation for a simple regression task, based on the MLP architecture discussed in the lecture exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self):\n",
    "        # Weights from the exercise sheet diagram\n",
    "        # W1: (3 inputs + 1 bias) x 2 hidden neurons\n",
    "        # W2: (2 hidden neurons + 1 bias) x 1 output neuron\n",
    "        # The exercise diagram implies weights are for connections, biases are separate\n",
    "        self.W1 = np.array([[0.1, 0.4],\n",
    "                              [0.2, 0.5],\n",
    "                              [0.3, 0.6]]) # Shape (3, 2)\n",
    "        self.b1 = np.zeros((1, 2)) # Shape (1, 2) for 2 hidden neurons\n",
    "\n",
    "        self.W2 = np.array([[0.7],\n",
    "                              [0.8]]) # Shape (2, 1)\n",
    "        self.b2 = np.zeros((1, 1)) # Shape (1, 1) for 1 output neuron\n",
    "\n",
    "        # Attributes to store intermediate values for backpropagation\n",
    "        self.z1 = None # Input to hidden layer activation\n",
    "        self.h1 = None # Output of hidden layer activation\n",
    "        self.z2 = None # Input to output layer activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is expected to be a column vector (e.g., shape (3,1) for 3 input features)\n",
    "        # Ensure x is a 2D array for consistent matrix multiplication\n",
    "        if x.ndim == 1:\n",
    "            x = x.reshape(-1, 1)\n",
    "        \n",
    "        # Hidden layer\n",
    "        # z = Wx + b. Here x is (num_features, 1), W1 is (num_hidden_units, num_features)\n",
    "        # To match exercise: W1 (features, hidden_units), x (features, 1). So x.T @ W1 or W1.T @ x\n",
    "        # Let's assume W1 is (input_dim, hidden_dim) and W2 is (hidden_dim, output_dim)\n",
    "        # Input x shape: (num_samples, num_input_features)\n",
    "        # self.W1 shape: (num_input_features, num_hidden_neurons)\n",
    "        # self.b1 shape: (1, num_hidden_neurons)\n",
    "        self.z1 = np.dot(x.T, self.W1) + self.b1 # z1 shape: (1, num_hidden_neurons)\n",
    "        self.h1 = sigmoid(self.z1) # h1 shape: (1, num_hidden_neurons)\n",
    "\n",
    "        # Output layer\n",
    "        # self.W2 shape: (num_hidden_neurons, num_output_neurons)\n",
    "        # self.b2 shape: (1, num_output_neurons)\n",
    "        self.z2 = np.dot(self.h1, self.W2) + self.b2 # z2 shape: (1, num_output_neurons)\n",
    "        y_hat = sigmoid(self.z2) # y_hat shape: (1, num_output_neurons)\n",
    "        \n",
    "        return y_hat\n",
    "\n",
    "    def backward(self, x, y, y_hat):\n",
    "        # x shape: (num_input_features, 1) or (num_samples, num_input_features)\n",
    "        # y, y_hat shape: (1, num_output_neurons) or (num_samples, num_output_neurons)\n",
    "        # Ensure x is a 2D array for consistent operations\n",
    "        if x.ndim == 1:\n",
    "            x = x.reshape(-1, 1)\n",
    "            \n",
    "        # Ensure y and y_hat are 2D arrays like (1,1)\n",
    "        if not isinstance(y, np.ndarray) or y.ndim == 0 or y.ndim == 1:\n",
    "            y = np.array([[y]])\n",
    "        if not isinstance(y_hat, np.ndarray) or y_hat.ndim == 0 or y_hat.ndim == 1:\n",
    "            y_hat = np.array([[y_hat]])\n",
    "\n",
    "        # MSE Loss: L = 0.5 * (y_hat - y)^2\n",
    "        # dL/dy_hat = y_hat - y\n",
    "        \n",
    "        # Output layer gradients (Layer 2)\n",
    "        # delta2 = dL/dz2 = dL/dy_hat * dy_hat/dz2\n",
    "        # dy_hat/dz2 = sigmoid_derivative(z2)\n",
    "        delta2 = (y_hat - y) * sigmoid_derivative(self.z2) # shape (1, num_output_neurons)\n",
    "\n",
    "        # dL/dW2 = dL/dz2 * dz2/dW2 = delta2 * h1.T\n",
    "        # self.h1 shape (1, num_hidden_neurons), delta2 shape (1, num_output_neurons)\n",
    "        # dW2 shape must be same as W2: (num_hidden_neurons, num_output_neurons)\n",
    "        dW2 = np.dot(self.h1.T, delta2) # (hidden, 1) @ (1, output) -> (hidden, output)\n",
    "\n",
    "        # dL/db2 = dL/dz2 * dz2/db2 = delta2 * 1\n",
    "        db2 = np.sum(delta2, axis=0, keepdims=True) # Sum over samples if batch > 1, here it's (1, output)\n",
    "\n",
    "        # Hidden layer gradients (Layer 1)\n",
    "        # delta1 = dL/dz1 = (dL/dz2 * dz2/dh1) * dh1/dz1\n",
    "        # dL/dz2 * dz2/dh1 = delta2 @ W2.T\n",
    "        # dh1/dz1 = sigmoid_derivative(z1)\n",
    "        # delta2 shape (1, num_output_neurons), self.W2.T shape (num_output_neurons, num_hidden_neurons)\n",
    "        delta1 = np.dot(delta2, self.W2.T) * sigmoid_derivative(self.z1) # shape (1, num_hidden_neurons)\n",
    "\n",
    "        # dL/dW1 = dL/dz1 * dz1/dW1 = delta1 * x.T\n",
    "        # x.T shape (1, num_input_features), delta1 shape (1, num_hidden_neurons)\n",
    "        # dW1 shape must be same as W1: (num_input_features, num_hidden_neurons)\n",
    "        # x has been transposed if it was a column vector for forward pass input\n",
    "        # If x was (features, 1), x.T is (1, features). Here we need (features, 1) @ (1, hidden)\n",
    "        dW1 = np.dot(x, delta1) # (features, 1) @ (1, hidden) -> (features, hidden)\n",
    "        \n",
    "        # dL/db1 = dL/dz1 * dz1/db1 = delta1 * 1\n",
    "        db1 = np.sum(delta1, axis=0, keepdims=True) # Sum over samples if batch > 1, here (1, hidden)\n",
    "\n",
    "        return dW2, db2, dW1, db1\n",
    "\n",
    "    def update_params(self, dW2, db2, dW1, db1, learning_rate):\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.b2 -= learning_rate * db2\n",
    "        self.W1 -= learning_rate * dW1\n",
    "        self.b1 -= learning_rate * db1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP()\n",
    "\n",
    "learning_rate = 0.1\n",
    "epochs = 1000\n",
    "\n",
    "# Training sample from the exercise (page with forward pass example)\n",
    "# x = [x1, x2, x3] = [36, 70, 1]\n",
    "# y = 1 (target output)\n",
    "x_train = np.array([[36], [70], [1]]) # Shape (3, 1)\n",
    "y_train = 1 # Target scalar value\n",
    "\n",
    "loss_history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    # The forward method expects x_train.T if x_train is (features, 1)\n",
    "    # Or it handles x_train directly if it's (1, features) or (features, 1) and transposes internally\n",
    "    # Let's make sure input to forward is (num_samples, num_features) = (1,3) for our single sample\n",
    "    # The current forward pass expects x.T to be (1,3) if W1 is (3,2)\n",
    "    # So x should be (3,1) which is current x_train shape.\n",
    "    y_hat = mlp.forward(x_train) # y_hat will be (1,1)\n",
    "    \n",
    "    # Calculate MSE loss\n",
    "    loss = 0.5 * (y_hat - y_train)**2\n",
    "    loss_history.append(loss.item()) # .item() to get scalar from (1,1) array\n",
    "    \n",
    "    # Backward pass\n",
    "    # backward expects x_train (3,1), y_train (scalar), y_hat (1,1)\n",
    "    dW2, db2, dW1, db1 = mlp.backward(x_train, y_train, y_hat)\n",
    "    \n",
    "    # Update parameters\n",
    "    mlp.update_params(dW2, db2, dW1, db1, learning_rate)\n",
    "    \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item():.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(epochs), loss_history)\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFinal Trained Weights and Biases:\")\n",
    "print(\"W1:\", mlp.W1)\n",
    "print(\"b1:\", mlp.b1)\n",
    "print(\"W2:\", mlp.W2)\n",
    "print(\"b2:\", mlp.b2)\n",
    "\n",
    "final_prediction = mlp.forward(x_train)\n",
    "print(f\"\\nFinal prediction for x_train = {x_train.T.tolist()}: {final_prediction.item():.6f}\")\n",
    "print(f\"Target value y_train: {y_train}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
